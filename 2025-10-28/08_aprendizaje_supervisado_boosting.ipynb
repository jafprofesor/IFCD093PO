{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236aa21a",
   "metadata": {},
   "source": [
    "# üöÄ M√≥dulo 2.5: M√©todos de Ensamblado - Boosting\n",
    "### Curso: **Machine Learning con Python** (IFCD093PO)\n",
    "**Duraci√≥n estimada:** 10 horas\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos del M√≥dulo\n",
    "\n",
    "Si Random Forest es un comit√© de expertos independientes, el Boosting es una l√≠nea de montaje donde cada experto aprende de los errores del anterior. En este m√≥dulo, dominar√°s el **Boosting**, la otra gran familia de m√©todos de ensamblado, conocida por su incre√≠ble rendimiento y por ser la opci√≥n preferida en competiciones de Machine Learning.\n",
    "\n",
    "Al finalizar, ser√°s capaz de:\n",
    "\n",
    "- ‚úÖ Entender el concepto de **aprendizaje secuencial** y c√≥mo los modelos de boosting se construyen para corregir errores pasados.\n",
    "- ‚úÖ Comprender el funcionamiento de **AdaBoost** y **Gradient Boosting**, los pilares del boosting.\n",
    "- ‚úÖ Implementar y optimizar los algoritmos de vanguardia: **XGBoost** y **LightGBM**.\n",
    "- ‚úÖ Conocer las ventajas clave de XGBoost y LightGBM, como la velocidad, la regularizaci√≥n y el manejo de datos faltantes.\n",
    "- ‚úÖ Distinguir claramente entre las filosof√≠as de **Bagging (Random Forest)** y **Boosting**.\n",
    "\n",
    "**¬°Prep√°rate para construir modelos que aprenden de sus errores para alcanzar la excelencia!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092e231",
   "metadata": {},
   "source": [
    "## üìö Tabla de Contenidos\n",
    "\n",
    "1. [Boosting: Aprendizaje Secuencial](#1-boosting)\n",
    "   - [La Idea: Corregir Errores Pasados](#1.1-idea)\n",
    "2. [AdaBoost (Adaptive Boosting)](#2-adaboost)\n",
    "   - [Poniendo Foco en los Casos Dif√≠ciles](#2.1-foco)\n",
    "   - [Implementaci√≥n de AdaBoost](#2.2-implementacion-ada)\n",
    "3. [Gradient Boosting](#3-gradient-boosting)\n",
    "   - [Aprendiendo de los Errores Residuales](#3.1-residuos)\n",
    "   - [Implementaci√≥n de Gradient Boosting](#3.2-implementacion-gb)\n",
    "4. [XGBoost: El Rey de las Competiciones](#4-xgboost)\n",
    "   - [¬øPor qu√© es tan Popular?](#4.1-popular)\n",
    "   - [Implementaci√≥n de XGBoost](#4.2-implementacion-xgb)\n",
    "5. [LightGBM: Velocidad y Eficiencia](#5-lightgbm)\n",
    "   - [Crecimiento por Hoja (Leaf-wise)](#5.1-leaf-wise)\n",
    "   - [Implementaci√≥n de LightGBM](#5.2-implementacion-lgbm)\n",
    "6. [Comparativa Final: Bagging vs. Boosting](#6-comparativa)\n",
    "7. [Resumen y Pr√≥ximos Pasos](#7-resumen)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab072e3",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è 1. Boosting: Aprendizaje Secuencial <a id='1-boosting'></a>\n",
    "\n",
    "### 1.1 La Idea: Corregir Errores Pasados <a id='1.1-idea'></a>\n",
    "\n",
    "A diferencia del Bagging (Random Forest) donde los √°rboles se entrenan en paralelo y de forma independiente, el **Boosting** es un m√©todo **secuencial**.\n",
    "\n",
    "**El proceso es el siguiente:**\n",
    "1.  Se entrena un primer modelo simple (un \"aprendiz d√©bil\", a menudo un √°rbol de decisi√≥n muy corto).\n",
    "2.  Se eval√∫an las predicciones de este modelo y se identifican sus errores.\n",
    "3.  Se entrena un **segundo modelo** que se enfoca espec√≠ficamente en **corregir los errores** del primer modelo.\n",
    "4.  Se entrena un tercer modelo que intenta corregir los errores combinados de los dos primeros.\n",
    "5.  Este proceso se repite `n` veces (el n√∫mero de estimadores).\n",
    "\n",
    "La predicci√≥n final es una **suma ponderada** de las predicciones de todos los modelos, donde los modelos que funcionaron mejor tienen m√°s peso.\n",
    "\n",
    "![Boosting](imagenes/Boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ec470",
   "metadata": {},
   "source": [
    "## üí™ 2. AdaBoost (Adaptive Boosting) <a id='2-adaboost'></a>\n",
    "\n",
    "### 2.1 Poniendo Foco en los Casos Dif√≠ciles <a id='2.1-foco'></a>\n",
    "\n",
    "AdaBoost fue uno de los primeros algoritmos de boosting y su idea es muy intuitiva.\n",
    "\n",
    "Funciona ajustando el **peso de las muestras de entrenamiento**. Inicialmente, todas las muestras tienen el mismo peso. Despu√©s de entrenar un aprendiz, AdaBoost **aumenta el peso** de las muestras que fueron **mal clasificadas**. En la siguiente iteraci√≥n, el nuevo aprendiz se ver√° forzado a prestar m√°s atenci√≥n a estos casos dif√≠ciles.\n",
    "\n",
    "Esto hace que el sistema se adapte (de ah√≠ \"Adaptive\") y se enfoque en las partes m√°s complejas del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bd00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de AdaBoost: 0.8800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier ## Importar AdaBoost\n",
    "from sklearn.tree import DecisionTreeClassifier ## Importar √°rbol de decisi√≥n\n",
    "from sklearn.datasets import make_moons ## Importar conjunto de datos de lunas\n",
    "from sklearn.model_selection import train_test_split ## Importar funci√≥n para dividir datos\n",
    "from sklearn.metrics import accuracy_score ## Importar m√©trica de precisi√≥n\n",
    "# Eliminamos las advertencias para mayor claridad\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Crear conjunto de datos de lunas\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# AdaBoost con 30 √°rboles de decisi√≥n de profundidad 1 (stumps)\n",
    "# Cada √°rbol contribuye con un learning_rate de 0.5\n",
    "# n_estimators define el n√∫mero de √°rboles en el ensemble\n",
    "# algorithm define el m√©todo de actualizaci√≥n de pesos\n",
    "# algorithm=\"SAMME\" es menos eficiente pero puede ser √∫til en algunos casos\n",
    "# otros valores posibles son \"SAMME.R\", que es m√°s eficiente para clasificaci√≥n binaria\n",
    "# El par√°metro algorithm de AdaBoostClassifier solo acepta 'SAMME' \n",
    "# como un valor v√°lido cuando se utiliza un estimador base \n",
    "# personalizado (como DecisionTreeClassifier). \n",
    "# El valor predeterminado 'SAMME.R' solo es compatible \n",
    "# cuando el estimador base admite estimaciones de probabilidad, \n",
    "# lo que no siempre es el caso para estimadores personalizados \n",
    "# en algunas versiones de scikit-learn.\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=30,\n",
    "    algorithm=\"SAMME\",\n",
    "    learning_rate=0.5, # El learning_rate controla cu√°nto contribuye cada √°rbol\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar y evaluar el modelo AdaBoost\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy de AdaBoost: {accuracy_score(y_test, y_pred_ada):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d28556",
   "metadata": {},
   "source": [
    "## üìâ 3. Gradient Boosting <a id='3-gradient-boosting'></a>\n",
    "\n",
    "### 3.1 Aprendiendo de los Errores Residuales <a id='3.1-residuos'></a>\n",
    "\n",
    "Gradient Boosting lleva la idea del boosting un paso m√°s all√°. En lugar de ajustar los pesos de las muestras, intenta ajustar los **errores residuales** del predictor anterior.\n",
    "\n",
    "**Proceso (para regresi√≥n):**\n",
    "1.  Se entrena un primer √°rbol de regresi√≥n simple sobre los datos.\n",
    "2.  Se calculan los **errores (residuales)**: `error = valor_real - predicci√≥n`.\n",
    "3.  Se entrena un **segundo √°rbol** para que **prediga esos errores**.\n",
    "4.  La nueva predicci√≥n combinada es `predicci√≥n_arbol_1 + learning_rate * predicci√≥n_arbol_2`.\n",
    "5.  Se calculan los nuevos residuales y se repite el proceso.\n",
    "\n",
    "El nombre \"Gradient\" viene de que este proceso es una forma de optimizaci√≥n por descenso de gradiente en el espacio de las funciones. Es un m√©todo extremadamente potente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7d45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE de Gradient Boosting: 0.4216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor ## Importar Gradient Boosting Regressor\n",
    "from sklearn.metrics import mean_squared_error ## Importar m√©trica de error cuadr√°tico medio\n",
    "import numpy as np ## Importar NumPy\n",
    "\n",
    "# Datos de ejemplo para regresi√≥n, creando una funci√≥n no lineal con ruido\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.rand(100, 1) * 10\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(100) * 0.5\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, random_state=42)\n",
    "\n",
    "# Entrenar un Gradient Boosting Regressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, learning_rate=0.1, random_state=42)\n",
    "gbrt.fit(X_train_r, y_train_r)\n",
    "\n",
    "y_pred_gbrt = gbrt.predict(X_test_r)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_gbrt))\n",
    "print(f\"RMSE de Gradient Boosting: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c1c2f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bee23",
   "metadata": {},
   "source": [
    "## üëë 4. XGBoost: El Rey de las Competiciones <a id='4-xgboost'></a>\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** es una implementaci√≥n optimizada y mejorada de Gradient Boosting. Durante a√±os, ha sido el algoritmo dominante en competiciones de Machine Learning como Kaggle.\n",
    "\n",
    "### 4.1 ¬øPor qu√© es tan Popular? <a id='4.1-popular'></a>\n",
    "\n",
    "1.  **Velocidad y Rendimiento**: Est√° implementado en C++ y es extremadamente r√°pido. Puede entrenar en paralelo (a nivel de caracter√≠sticas).\n",
    "2.  **Regularizaci√≥n Incorporada**: Incluye regularizaci√≥n L1 (Lasso) y L2 (Ridge) en su funci√≥n de coste, lo que lo hace muy robusto contra el overfitting.\n",
    "3.  **Manejo de Datos Faltantes**: Puede manejar valores nulos de forma nativa.\n",
    "4.  **Validaci√≥n Cruzada Integrada**: Puede realizar validaci√≥n cruzada en cada iteraci√≥n.\n",
    "5.  **Poda de √Årboles Avanzada**: Realiza una poda m√°s inteligente que el Gradient Boosting est√°ndar.\n",
    "\n",
    "XGBoost no est√° incluido en Scikit-learn por defecto, por lo que necesita ser instalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206643de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de XGBoost: 0.8640\n"
     ]
    }
   ],
   "source": [
    "# Para instalar XGBoost, ejecuta en tu terminal:\n",
    "# pip install xgboost\n",
    "\n",
    "import xgboost # Importar XGBoost una vez instalado\n",
    "\n",
    "# Entrenar un clasificador XGBoost\n",
    "# Los parametros son:\n",
    "# random_state para reproducibilidad\n",
    "# use_label_encoder=False para evitar advertencias sobre el codificador de etiquetas\n",
    "# eval_metric='logloss' para definir la m√©trica de evaluaci√≥n\n",
    "xgb_clf = xgboost.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(f\"Accuracy de XGBoost: {accuracy_score(y_test, y_pred_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69366ac4",
   "metadata": {},
   "source": [
    "## üí° 5. LightGBM: Velocidad y Eficiencia <a id='5-lightgbm'></a>\n",
    "\n",
    "**LightGBM (Light Gradient Boosting Machine)** es otro framework de boosting desarrollado por Microsoft. Su principal ventaja es su **velocidad y eficiencia en datasets muy grandes**.\n",
    "\n",
    "### 5.1 Crecimiento por Hoja (Leaf-wise) <a id='5.1-leaf-wise'></a>\n",
    "\n",
    "La principal diferencia con XGBoost es c√≥mo crecen los √°rboles:\n",
    "- **XGBoost** crece por **nivel (level-wise)**: Completa un nivel entero del √°rbol antes de pasar al siguiente. Es m√°s sistem√°tico.\n",
    "- **LightGBM** crece por **hoja (leaf-wise)**: Expande la hoja que m√°s reduce la p√©rdida, lo que lleva a √°rboles m√°s asim√©tricos pero a menudo m√°s eficientes.\n",
    "\n",
    "![Leaf-wise](imagenes/Leaf-wise.png)\n",
    "\n",
    "Esto hace que LightGBM sea extremadamente r√°pido, aunque a veces puede ser m√°s propenso al overfitting en datasets peque√±os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5720c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 186, number of negative: 189\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 250\n",
      "[LightGBM] [Info] Number of data points in the train set: 375, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496000 -> initscore=-0.016000\n",
      "[LightGBM] [Info] Start training from score -0.016000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy de LightGBM: 0.8880\n"
     ]
    }
   ],
   "source": [
    "# Para instalar LightGBM, ejecuta en tu terminal:\n",
    "# pip install lightgbm\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "lgbm_clf = lightgbm.LGBMClassifier(random_state=42)\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test)\n",
    "print(f\"Accuracy de LightGBM: {accuracy_score(y_test, y_pred_lgbm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1618a",
   "metadata": {},
   "source": [
    "Esta salida de LightGBM muestra informaci√≥n y advertencias del proceso de entrenamiento. Lo m√°s importante es la advertencia repetida:\n",
    "\n",
    "*[LightGBM] [Warning] No further splits with positive gain, best gain: -inf*\n",
    "\n",
    "Esto significa que, para los datos y par√°metros usados, LightGBM no encuentra ninguna divisi√≥n (split) en los √°rboles que mejore la funci√≥n objetivo (ganancia positiva). Por eso, los √°rboles que construye son muy simples (a veces solo la ra√≠z).\n",
    "\n",
    "Las causas m√°s comunes son:\n",
    "\n",
    "- **Muy pocas variables/features:** En tu salida, dice \"number of used features: 2\". Con solo 2 variables, es f√°cil que no haya splits √∫tiles.\n",
    "- **Datos poco variados o mal preparados:** Si las variables no tienen suficiente variabilidad o no est√°n bien escaladas/categorizadas, el modelo no puede aprender.\n",
    "- **Par√°metros restrictivos:** Si usas un valor muy bajo de max_depth, min_data_in_leaf alto, o min_gain_to_split alto, puede impedir que se hagan splits.\n",
    "\n",
    "**¬øEs un error grave?**\n",
    "No es un error cr√≠tico si el accuracy es bueno, pero indica que el modelo no est√° aprovechando la estructura de los datos. Si ves muchas advertencias y el accuracy es bajo, revisa tus datos y par√°metros.\n",
    "\n",
    "**¬øQu√© hacer?**\n",
    "\n",
    "- Aseg√∫rate de tener suficientes variables predictoras.\n",
    "- Revisa la preparaci√≥n de los datos.\n",
    "- Ajusta los hiperpar√°metros (por ejemplo, baja min_gain_to_split, baja min_data_in_leaf, aumenta max_depth).\n",
    "- Si solo tienes 2 variables, prueba a√±adiendo m√°s o usando otro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b493ba3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688afe",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 6. Comparativa Final: Bagging vs. Boosting <a id='6-comparativa'></a>\n",
    "\n",
    "| Caracter√≠stica | Bagging (Random Forest) | Boosting (Gradient Boosting, XGBoost) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Filosof√≠a** | Entrena modelos **independientes** en paralelo. | Entrena modelos **secuencialmente**, aprendiendo de errores. |\n",
    "| **Objetivo Principal** | Reducir la **varianza** (overfitting). | Reducir el **sesgo** (underfitting) y la varianza. |\n",
    "| **Rendimiento** | Muy bueno y robusto. | A menudo, el **mejor rendimiento** posible (state-of-the-art). |\n",
    "| **Sensibilidad** | Menos sensible a los hiperpar√°metros. | M√°s sensible a los hiperpar√°metros (`learning_rate`, etc.). |\n",
    "| **Velocidad** | Puede ser paralelizado f√°cilmente. | Es secuencial, pero XGBoost/LightGBM son muy r√°pidos. |\n",
    "| **Primer Intento** | Excelente como primer modelo a probar. | Excelente para exprimir el m√°ximo rendimiento. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f8be4",
   "metadata": {},
   "source": [
    "## üìù 7. Resumen y Pr√≥ximos Pasos <a id='7-resumen'></a>\n",
    "\n",
    "### üéâ ¬°Has llegado a la cima del Aprendizaje Supervisado!\n",
    "\n",
    "#### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "1. **Filosof√≠a del Boosting**\n",
    "   - El poder del **aprendizaje secuencial**, donde cada modelo se enfoca en corregir los errores de los anteriores.\n",
    "\n",
    "2. **Algoritmos Clave**\n",
    "   - **AdaBoost**, que ajusta los pesos de las muestras para centrarse en los casos dif√≠ciles.\n",
    "   - **Gradient Boosting**, que entrena nuevos modelos para predecir los errores residuales de los anteriores.\n",
    "\n",
    "3. **Implementaciones de Vanguardia**\n",
    "   - **XGBoost** y **LightGBM**, las librer√≠as optimizadas que dominan el Machine Learning pr√°ctico por su velocidad, regularizaci√≥n y rendimiento superior.\n",
    "\n",
    "4. **Bagging vs. Boosting**\n",
    "   - La diferencia fundamental entre reducir la varianza (Bagging) y reducir el sesgo (Boosting).\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Pr√≥ximo M√≥dulo: Introducci√≥n al Aprendizaje No Supervisado\n",
    "\n",
    "Has completado un viaje exhaustivo por el Aprendizaje Supervisado. Ahora, es el momento de cambiar de paradigma. ¬øQu√© hacemos cuando **no tenemos etiquetas**? ¬øCu√°ndo no hay una \"respuesta correcta\" que aprender?\n",
    "\n",
    "En el pr√≥ximo m√≥dulo, nos adentraremos en el **Aprendizaje No Supervisado**, donde el objetivo es encontrar patrones, estructura y conocimiento oculto en los datos por s√≠ mismos. Empezaremos con una introducci√≥n a este nuevo y emocionante campo.\n",
    "\n",
    "**Has aprendido a predecir el futuro bas√°ndote en el pasado. Ahora, ¬°vamos a descubrir la estructura oculta del presente!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

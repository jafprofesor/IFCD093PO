{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6d48e7",
   "metadata": {},
   "source": [
    "# üå≤ M√≥dulo 2.4: M√©todos de Ensamblado - Random Forest\n",
    "### Curso: **Machine Learning con Python** (IFCD093PO)\n",
    "**Duraci√≥n estimada:** 10 horas\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivos del M√≥dulo\n",
    "\n",
    "Un solo √°rbol es bueno, pero un bosque es m√°s fuerte. En este m√≥dulo, descubrir√°s el poder de los **m√©todos de ensamblado (Ensemble Methods)**, una de las t√©cnicas m√°s potentes en Machine Learning. Nos centraremos en el **Random Forest**, un algoritmo que soluciona la principal debilidad de los √°rboles de decisi√≥n: su tendencia al overfitting.\n",
    "\n",
    "Al finalizar, ser√°s capaz de:\n",
    "\n",
    "- ‚úÖ Entender la \"sabidur√≠a de la multitud\" y por qu√© los ensamblados superan a los modelos individuales.\n",
    "- ‚úÖ Dominar la t√©cnica de **Bagging (Bootstrap Aggregating)**, el pilar fundamental de Random Forest.\n",
    "- ‚úÖ Entrenar, evaluar y optimizar modelos de **Random Forest** para clasificaci√≥n y regresi√≥n.\n",
    "- ‚úÖ Utilizar la **Importancia de Caracter√≠sticas (Feature Importance)** para interpretar qu√© variables son m√°s influyentes en tu modelo Random Forest.\n",
    "- ‚úÖ Comprender el papel del par√°metro `n_estimators` y c√≥mo afecta al rendimiento.\n",
    "\n",
    "**¬°Prep√°rate para dejar de confiar en un solo experto y empezar a escuchar a un comit√© de ellos!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fa3e5",
   "metadata": {},
   "source": [
    "## üìö Tabla de Contenidos\n",
    "\n",
    "1. [La Sabidur√≠a de la Multitud: M√©todos de Ensamblado](#1-ensamblado)\n",
    "   - [¬øPor qu√© un Ensamblado es Mejor?](#1.1-porque)\n",
    "2. [Bagging: Bootstrap Aggregating](#2-bagging)\n",
    "   - [Paso a Paso: C√≥mo Funciona el Bagging](#2.1-pasos)\n",
    "3. [Random Forest: El Bosque Aleatorio](#3-random-forest)\n",
    "   - [A√±adiendo Aleatoriedad a Bagging](#3.1-aleatoriedad)\n",
    "   - [Implementaci√≥n de Random Forest para Clasificaci√≥n](#3.2-implementacion-clas)\n",
    "   - [Random Forest para Regresi√≥n](#3.3-implementacion-reg)\n",
    "4. [Interpretaci√≥n: Importancia de Caracter√≠sticas](#4-importancia)\n",
    "   - [¬øQu√© Variables Mira m√°s el Bosque?](#4.1-variables)\n",
    "5. [Ventajas y Desventajas de Random Forest](#5-ventajas)\n",
    "6. [Resumen y Pr√≥ximos Pasos](#6-resumen)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677fb3b",
   "metadata": {},
   "source": [
    "## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ 1. La Sabidur√≠a de la Multitud: M√©todos de Ensamblado <a id='1-ensamblado'></a>\n",
    "\n",
    "### 1.1 ¬øPor qu√© un Ensamblado es Mejor? <a id='1.1-porque'></a>\n",
    "\n",
    "La idea central de los m√©todos de ensamblado es simple: **combinar las predicciones de varios modelos (llamados \"aprendices d√©biles\" o \"weak learners\") para crear un √∫nico modelo superior (un \"aprendiz fuerte\" o \"strong learner\")**.\n",
    "\n",
    "**Analog√≠a**: Imagina que est√°s en un concurso. En lugar de responder t√∫ solo, puedes preguntar a 100 personas y quedarte con la respuesta m√°s votada. La probabilidad de que la mayor√≠a se equivoque es mucho menor que la probabilidad de que una sola persona se equivoque.\n",
    "\n",
    "Un ensamblado funciona si los modelos que lo componen son:\n",
    "1.  **Diversos**: Cometen errores en diferentes sitios. Si todos se equivocan en lo mismo, el ensamblado tambi√©n se equivocar√°.\n",
    "2.  **Mejores que el azar**: Cada modelo individual debe tener un rendimiento al menos ligeramente superior al de una predicci√≥n aleatoria.\n",
    "\n",
    "Los √°rboles de decisi√≥n son candidatos perfectos para ser los aprendices d√©biles en un ensamblado, ya que son muy sensibles a los datos de entrenamiento (alta varianza), lo que facilita la creaci√≥n de √°rboles diversos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90db4a",
   "metadata": {},
   "source": [
    "## üõçÔ∏è 2. Bagging: Bootstrap Aggregating <a id='2-bagging'></a>\n",
    "\n",
    "El **Bagging** es una de las t√©cnicas de ensamblado m√°s importantes y es la base del Random Forest.\n",
    "\n",
    "### 2.1 Paso a Paso: C√≥mo Funciona el Bagging <a id='2.1-pasos'></a>\n",
    "\n",
    "1.  **Bootstrap (Muestreo con Reemplazo)**: Se crean m√∫ltiples subconjuntos de datos a partir del conjunto de entrenamiento original. Cada subconjunto tiene el mismo tama√±o que el original, pero se crea **muestreando con reemplazo**. Esto significa que algunas muestras pueden aparecer varias veces en un subconjunto, mientras que otras pueden no aparecer en absoluto.\n",
    "\n",
    "2.  **Training (Entrenamiento Paralelo)**: Se entrena un modelo (un √°rbol de decisi√≥n, en nuestro caso) de forma independiente en cada uno de los subconjuntos de datos. Como cada √°rbol ve una versi√≥n ligeramente diferente de los datos, aprender√° patrones ligeramente diferentes, creando la **diversidad** que necesitamos.\n",
    "\n",
    "3.  **Aggregating (Agregaci√≥n)**: Para hacer una nueva predicci√≥n, se le pregunta a todos los √°rboles del ensamblado. \n",
    "    -   En **clasificaci√≥n**, la predicci√≥n final es la clase m√°s votada (**votaci√≥n por mayor√≠a**).\n",
    "    -   En **regresi√≥n**, la predicci√≥n final es el **promedio** de las predicciones de todos los √°rboles.\n",
    "\n",
    "![Bagging](imagenes/Bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff24099",
   "metadata": {},
   "source": [
    "## üå≤ 3. Random Forest: El Bosque Aleatorio <a id='3-random-forest'></a>\n",
    "\n",
    "### 3.1 A√±adiendo Aleatoriedad a Bagging <a id='3.1-aleatoriedad'></a>\n",
    "\n",
    "Un Random Forest es, en esencia, un modelo de Bagging de √°rboles de decisi√≥n con un toque extra de aleatoriedad para aumentar la diversidad de los √°rboles.\n",
    "\n",
    "**¬øCu√°l es el truco?**\n",
    "\n",
    "Cuando un √°rbol del bosque est√° decidiendo en qu√© caracter√≠stica y umbral dividir un nodo, **no considera todas las caracter√≠sticas disponibles**. En su lugar, selecciona un **subconjunto aleatorio de caracter√≠sticas** y solo busca la mejor divisi√≥n entre ellas.\n",
    "\n",
    "**¬øPor qu√© hacer esto?**\n",
    "\n",
    "Imagina que tienes una caracter√≠stica muy predictiva. En un modelo de Bagging normal, la mayor√≠a de los √°rboles empezar√≠an dividiendo por esa caracter√≠stica, lo que har√≠a que todos los √°rboles fueran muy similares (correlacionados). Al forzar a cada √°rbol a considerar solo un subconjunto aleatorio de caracter√≠sticas, nos aseguramos de que los √°rboles sean m√°s diferentes entre s√≠, lo que **reduce la varianza** y hace que el modelo final sea m√°s robusto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f953bd",
   "metadata": {},
   "source": [
    "### 3.2 Implementaci√≥n de Random Forest para Clasificaci√≥n <a id='3.2-implementacion-clas'></a>\n",
    "\n",
    "Vamos a usar el dataset de `make_moons` para ver c√≥mo un Random Forest puede superar a un √∫nico √°rbol de decisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13449df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons # Generar un conjunto de datos de \"lunas\"\n",
    "from sklearn.model_selection import train_test_split # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "from sklearn.ensemble import RandomForestClassifier # Importar el clasificador Random Forest\n",
    "from sklearn.tree import DecisionTreeClassifier # Importar el clasificador √Årbol de Decisi√≥n\n",
    "from sklearn.metrics import accuracy_score # Medir la precisi√≥n del modelo\n",
    "import matplotlib.pyplot as plt # Para visualizaci√≥n\n",
    "import numpy as np # Para operaciones num√©ricas\n",
    "\n",
    "# Generar datos de \"lunas\" para clasificaci√≥n, esto crea un conjunto de datos no linealmente separable\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Entrenar un √∫nico √Årbol de Decisi√≥n \n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(f\"Accuracy de un solo √Årbol de Decisi√≥n: {accuracy_score(y_test, y_pred_tree):.4f}\")\n",
    "\n",
    "# Entrenar un Random Forest\n",
    "# n_estimators es el n√∫mero de √°rboles en el bosque, n_estimators=100 es un valor com√∫nmente usado \n",
    "# y nos dice que usemos 100 √°rboles, cada uno entrenado con una muestra aleatoria del conjunto de datos\n",
    "# n_jobs=-1 indica que se usen todos los n√∫cleos del procesador para acelerar el entrenamiento\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 usa todos los cores\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(f\"Accuracy de Random Forest (100 √°rboles): {accuracy_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para visualizar los l√≠mites de decisi√≥n\n",
    "# Aqui vamos a crear una malla de puntos en el espacio de caracter√≠sticas \n",
    "# y predecir la clase para cada punto\n",
    "# Luego, usamos estos puntos para dibujar los l√≠mites de decisi√≥n del clasificador\n",
    "# Esto nos ayuda a ver c√≥mo el modelo clasifica diferentes regiones del espacio \n",
    "# de caracter√≠sticas\n",
    "def plot_decision_boundary(clf, X, y, axes, title):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.RdYlBu, alpha=0.5)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"b.\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"r.\")\n",
    "    plt.title(title)\n",
    "    plt.axis(axes)\n",
    "\n",
    "fig, axes_fig = plt.subplots(1, 2, figsize=(12, 5))\n",
    "plt.sca(axes_fig[0])\n",
    "plot_decision_boundary(tree_clf, X, y, [-1.5, 2.5, -1, 1.5], \"√Årbol de Decisi√≥n\")\n",
    "plt.sca(axes_fig[1])\n",
    "plot_decision_boundary(rf_clf, X, y, [-1.5, 2.5, -1, 1.5], \"Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23190d83",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "- El l√≠mite de decisi√≥n del √∫nico √°rbol es irregular y claramente sobreajustado a los datos de entrenamiento (alta varianza).\n",
    "- El l√≠mite de decisi√≥n del Random Forest es mucho m√°s suave y general. Al promediar las predicciones de muchos √°rboles, se reduce el ruido y se obtiene un modelo m√°s robusto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936e063",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest para Regresi√≥n <a id='3.3-implementacion-reg'></a>\n",
    "\n",
    "El concepto es exactamente el mismo para regresi√≥n. Se entrenan m√∫ltiples √°rboles de regresi√≥n en subconjuntos de datos, y la predicci√≥n final es el **promedio** de las predicciones de todos los √°rboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # Importar el regresor Random Forest\n",
    "from sklearn.metrics import mean_squared_error # Medir el error cuadr√°tico medio\n",
    "\n",
    "# Datos de ejemplo para regresi√≥n\n",
    "# Generar datos sint√©ticos para regresi√≥n\n",
    "# Aqu√≠ usamos una funci√≥n seno con algo de ruido\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.rand(100, 1) * 10\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(100) * 0.5\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, random_state=42)\n",
    "\n",
    "# Entrenar un Random Forest Regressor\n",
    "# n_estimators es el n√∫mero de √°rboles en el bosque\n",
    "# n_jobs=-1 usa todos los cores para acelerar el entrenamiento\n",
    "# Si n_jobs no se especifica, el valor por defecto es 1, \n",
    "# lo que significa que solo se usa un n√∫cleo del procesador\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_reg.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Predecir y evaluar el modelo\n",
    "y_pred_reg = rf_reg.predict(X_test_r)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_reg))\n",
    "print(f\"RMSE del Random Forest Regressor: {rmse:.4f}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_plot = rf_reg.predict(X_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train_r, y_train_r, c='b', s=20, label='Datos de entrenamiento')\n",
    "plt.plot(X_plot, y_plot, c='r', lw=3, label='Predicci√≥n Random Forest')\n",
    "plt.title('Random Forest para Regresi√≥n')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7237e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c1403",
   "metadata": {},
   "source": [
    "## üìä 4. Interpretaci√≥n: Importancia de Caracter√≠sticas <a id='4-importancia'></a>\n",
    "\n",
    "### 4.1 ¬øQu√© Variables Mira m√°s el Bosque? <a id='4.1-variables'></a>\n",
    "\n",
    "Aunque un Random Forest es m√°s una \"caja negra\" que un solo √°rbol, nos ofrece una herramienta muy √∫til para la interpretaci√≥n: la **importancia de caracter√≠sticas (feature importance)**.\n",
    "\n",
    "**¬øC√≥mo se calcula?**\n",
    "\n",
    "Para cada caracter√≠stica, se mide cu√°nto ha contribuido en promedio a **reducir la impureza** en todos los √°rboles del bosque. Una caracter√≠stica que se usa a menudo en las divisiones de los nodos superiores (cerca de la ra√≠z) y que produce grandes ganancias de informaci√≥n tendr√° una alta importancia.\n",
    "\n",
    "Esto es extremadamente √∫til para entender qu√© variables son las m√°s predictivas en nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # Cargar el dataset Iris\n",
    "import pandas as pd\n",
    "\n",
    "# Usar el dataset Iris, que tiene caracter√≠sticas con nombres\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Entrenar un Random Forest en el dataset Iris\n",
    "rf_iris = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_iris.fit(X_iris, y_iris)\n",
    "\n",
    "# Obtener la importancia de las caracter√≠sticas\n",
    "# La importancia de las caracter√≠sticas indica qu√© tan √∫tiles son las caracter√≠sticas \n",
    "# individuales para hacer predicciones\n",
    "importances = rf_iris.feature_importances_\n",
    "\n",
    "# Crear un DataFrame para visualizarlo mejor\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Caracter√≠stica': iris.feature_names,\n",
    "    'Importancia': importances\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"Importancia de las Caracter√≠sticas (Dataset Iris):\")\n",
    "display(feature_importance_df)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Caracter√≠stica'], feature_importance_df['Importancia'])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Importancia de Caracter√≠sticas seg√∫n Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1fc61",
   "metadata": {},
   "source": [
    "Claramente, las caracter√≠sticas relacionadas con los p√©talos (`petal width` y `petal length`) son mucho m√°s importantes para clasificar las flores Iris que las relacionadas con los s√©palos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ded7a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a5388",
   "metadata": {},
   "source": [
    "## ‚úÖ 5. Ventajas y Desventajas de Random Forest <a id='5-ventajas'></a>\n",
    "\n",
    "### Ventajas\n",
    "1.  **Alto Rendimiento**: Es uno de los mejores algoritmos \"listos para usar\". Generalmente ofrece un rendimiento muy bueno con poca optimizaci√≥n de hiperpar√°metros.\n",
    "2.  **Robusto al Overfitting**: La combinaci√≥n de Bagging y la selecci√≥n aleatoria de caracter√≠sticas lo hace mucho menos propenso al overfitting que un solo √°rbol.\n",
    "3.  **No Requiere Escalado de Datos**: Al igual que los √°rboles individuales, no es sensible a la escala de las caracter√≠sticas.\n",
    "4.  **Maneja Datos Faltantes**: Puede manejar valores nulos de forma robusta.\n",
    "5.  **Proporciona Importancia de Caracter√≠sticas**: Permite interpretar qu√© variables son m√°s predictivas.\n",
    "\n",
    "### Desventajas\n",
    "1.  **Menos Interpretable**: Es una \"caja negra\". No podemos visualizar un bosque de 500 √°rboles como lo har√≠amos con uno solo.\n",
    "2.  **M√°s Lento**: Entrenar y predecir con cientos de √°rboles es m√°s costoso computacionalmente que con un solo modelo.\n",
    "3.  **Puede no ser la mejor opci√≥n para datos muy esparsos** (muchos ceros), como los datos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654bf74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba98756",
   "metadata": {},
   "source": [
    "## üìù 6. Resumen y Pr√≥ximos Pasos <a id='6-resumen'></a>\n",
    "\n",
    "### üéâ ¬°Has construido tu primer bosque! Has dominado uno de los algoritmos m√°s potentes y vers√°tiles del Machine Learning.\n",
    "\n",
    "#### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "1. **El Poder del Ensamblado**\n",
    "   - Que combinar muchos modelos d√©biles y diversos crea un modelo fuerte y robusto.\n",
    "   - La t√©cnica de **Bagging**, que entrena modelos en subconjuntos de datos muestreados con reemplazo.\n",
    "\n",
    "2. **Random Forest**\n",
    "   - Que un Random Forest es un ensamblado de √°rboles de decisi√≥n que a√±ade una capa extra de aleatoriedad al seleccionar caracter√≠sticas.\n",
    "   - C√≥mo esta doble aleatoriedad combate eficazmente el overfitting.\n",
    "   - A implementar `RandomForestClassifier` y `RandomForestRegressor` en Scikit-learn.\n",
    "\n",
    "3. **Interpretabilidad**\n",
    "   - A extraer y visualizar la **importancia de las caracter√≠sticas** para entender qu√© variables son las m√°s predictivas, abriendo una peque√±a ventana en la \"caja negra\".\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Pr√≥ximo M√≥dulo: Boosting - El Otro Gran Ensamblado\n",
    "\n",
    "El Bagging (y Random Forest) construye los √°rboles en paralelo, de forma independiente. Pero, ¬øy si los construy√©ramos en secuencia, donde cada nuevo √°rbol intenta corregir los errores del anterior?\n",
    "\n",
    "Esa es la idea detr√°s del **Boosting**, la otra gran familia de m√©todos de ensamblado. En el pr√≥ximo m√≥dulo, explorar√°s:\n",
    "\n",
    "- **AdaBoost** y **Gradient Boosting**: Los algoritmos fundamentales de boosting.\n",
    "- **XGBoost** y **LightGBM**: Las implementaciones modernas y ultra optimizadas que dominan las competiciones de Machine Learning (como Kaggle).\n",
    "\n",
    "**Has aprendido a construir un bosque donde cada √°rbol crece por su cuenta. Ahora, ¬°vamos a ense√±ar a los √°rboles a aprender de los errores de sus predecesores!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

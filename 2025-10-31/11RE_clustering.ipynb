{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffedc1c",
   "metadata": {},
   "source": [
    "# 游빌 Ejemplos Reales de Clustering\n",
    "\n",
    "En este notebook aplicaremos los algoritmos de clustering aprendidos (K-Means, Clustering Jer치rquico y DBSCAN) sobre datasets reales. Analizaremos los resultados, visualizaremos los cl칰steres y mostraremos c칩mo el clustering puede ayudarnos a descubrir patrones y detectar anomal칤as en datos reales.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Importar librer칤as y cargar datos reales\n",
    "\n",
    "En primer lugar, importamos las librer칤as necesarias y cargamos un dataset real. Usaremos el famoso dataset **Iris** y el dataset **Wine** de UCI, ambos disponibles en Scikit-Learn. Puedes adaptar el c칩digo para tus propios datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00893ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer칤as\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Cargar dataset Iris\n",
    "data_iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(data_iris.data, columns=data_iris.feature_names)\n",
    "df_iris['target'] = data_iris.target\n",
    "\n",
    "# Cargar dataset Wine\n",
    "data_wine = datasets.load_wine()\n",
    "df_wine = pd.DataFrame(data_wine.data, columns=data_wine.feature_names)\n",
    "df_wine['target'] = data_wine.target\n",
    "\n",
    "# Mostrar primeras filas de ambos datasets\n",
    "display(df_iris.head())\n",
    "display(df_wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d2649",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Preprocesamiento de datos reales\n",
    "\n",
    "Antes de aplicar clustering, es fundamental preparar los datos: eliminar valores nulos, seleccionar variables relevantes y escalar las caracter칤sticas. Adem치s, usaremos PCA para reducir la dimensionalidad y facilitar la visualizaci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c61331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento para Iris\n",
    "df_iris_clean = df_iris.dropna()\n",
    "X_iris = df_iris_clean.drop('target', axis=1)\n",
    "scaler_iris = StandardScaler()\n",
    "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
    "\n",
    "# Preprocesamiento para Wine\n",
    "df_wine_clean = df_wine.dropna()\n",
    "X_wine = df_wine_clean.drop('target', axis=1)\n",
    "scaler_wine = StandardScaler()\n",
    "X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "\n",
    "# Reducci칩n de dimensionalidad para visualizaci칩n (PCA a 2D)\n",
    "pca_iris = PCA(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "pca_wine = PCA(n_components=2)\n",
    "X_wine_pca = pca_wine.fit_transform(X_wine_scaled)\n",
    "\n",
    "# Visualizaci칩n r치pida de los datos reales en 2D\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].scatter(X_iris_pca[:,0], X_iris_pca[:,1], c=df_iris_clean['target'], cmap='viridis', s=40)\n",
    "axes[0].set_title('Iris (PCA 2D)')\n",
    "axes[1].scatter(X_wine_pca[:,0], X_wine_pca[:,1], c=df_wine_clean['target'], cmap='tab10', s=40)\n",
    "axes[1].set_title('Wine (PCA 2D)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867abbb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Clustering con K-Means en datos reales\n",
    "\n",
    "Aplicamos K-Means a los datasets Iris y Wine. Visualizaremos los cl칰steres en el espacio reducido por PCA y analizaremos la calidad del agrupamiento con la m칠trica Silhouette Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc016354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means en Iris\n",
    "kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "labels_iris = kmeans_iris.fit_predict(X_iris_scaled)\n",
    "silhouette_iris = silhouette_score(X_iris_scaled, labels_iris)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], c=labels_iris, cmap='Set1', s=40)\n",
    "plt.title(f'Iris - K-Means (Silhouette: {silhouette_iris:.2f})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()\n",
    "\n",
    "# K-Means en Wine\n",
    "kmeans_wine = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "labels_wine = kmeans_wine.fit_predict(X_wine_scaled)\n",
    "silhouette_wine = silhouette_score(X_wine_scaled, labels_wine)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_wine_pca[:,0], X_wine_pca[:,1], c=labels_wine, cmap='Set2', s=40)\n",
    "plt.title(f'Wine - K-Means (Silhouette: {silhouette_wine:.2f})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c60bed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Clustering Jer치rquico en datos reales\n",
    "\n",
    "Ahora aplicamos el clustering jer치rquico aglomerativo a los mismos datos. Visualizaremos el dendrograma y los cl칰steres resultantes, y los compararemos con los de K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrograma para Iris\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Dendrograma - Iris')\n",
    "dend = sch.dendrogram(sch.linkage(X_iris_scaled, method='ward'))\n",
    "plt.xlabel('칈ndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# Clustering Jer치rquico en Iris\n",
    "agg_iris = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels_agg_iris = agg_iris.fit_predict(X_iris_scaled)\n",
    "silhouette_agg_iris = silhouette_score(X_iris_scaled, labels_agg_iris)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], c=labels_agg_iris, cmap='Set1', s=40)\n",
    "plt.title(f'Iris - Jer치rquico (Silhouette: {silhouette_agg_iris:.2f})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()\n",
    "\n",
    "# Dendrograma para Wine\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Dendrograma - Wine')\n",
    "dend = sch.dendrogram(sch.linkage(X_wine_scaled, method='ward'))\n",
    "plt.xlabel('칈ndice de muestra')\n",
    "plt.ylabel('Distancia')\n",
    "plt.show()\n",
    "\n",
    "# Clustering Jer치rquico en Wine\n",
    "agg_wine = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels_agg_wine = agg_wine.fit_predict(X_wine_scaled)\n",
    "silhouette_agg_wine = silhouette_score(X_wine_scaled, labels_agg_wine)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_wine_pca[:,0], X_wine_pca[:,1], c=labels_agg_wine, cmap='Set2', s=40)\n",
    "plt.title(f'Wine - Jer치rquico (Silhouette: {silhouette_agg_wine:.2f})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429de43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Clustering con DBSCAN en datos reales\n",
    "\n",
    "Probamos DBSCAN en los mismos datos. Ajustaremos los hiperpar치metros y analizaremos los puntos de ruido detectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN en Iris\n",
    "# Ajusta eps y min_samples seg칰n el dataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Curva k-distance para estimar eps en Iris\n",
    "distances_iris = NearestNeighbors(n_neighbors=5).fit(X_iris_scaled).kneighbors(X_iris_scaled)[0][:, -1]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(np.sort(distances_iris))\n",
    "plt.title('Curva k-distance (Iris)')\n",
    "plt.xlabel('Puntos ordenados')\n",
    "plt.ylabel('Distancia al 5췈 vecino')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# DBSCAN\n",
    "dbscan_iris = DBSCAN(eps=0.6, min_samples=5)\n",
    "labels_dbscan_iris = dbscan_iris.fit_predict(X_iris_scaled)\n",
    "silhouette_dbscan_iris = silhouette_score(X_iris_scaled, labels_dbscan_iris, metric='euclidean') if len(set(labels_dbscan_iris)) > 1 else -1\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], c=labels_dbscan_iris, cmap='tab10', s=40)\n",
    "plt.title(f'Iris - DBSCAN (Silhouette: {silhouette_dbscan_iris:.2f})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()\n",
    "\n",
    "# DBSCAN en Wine\n",
    "distances_wine = NearestNeighbors(n_neighbors=5).fit(X_wine_scaled).kneighbors(X_wine_scaled)[0][:, -1]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(np.sort(distances_wine))\n",
    "plt.title('Curva k-distance (Wine)')\n",
    "plt.xlabel('Puntos ordenados')\n",
    "plt.ylabel('Distancia al 5췈 vecino')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "dbscan_wine = DBSCAN(eps=0.7, min_samples=5)\n",
    "labels_dbscan_wine = dbscan_wine.fit_predict(X_wine_scaled)\n",
    "silhouette_dbscan_wine = silhouette_score(X_wine_scaled, labels_dbscan_wine, metric='euclidean') if len(set(labels_dbscan_wine)) > 1 else -1\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_wine_pca[:,0], X_wine_pca[:,1], c=labels_dbscan_wine, cmap='tab20', s=40)\n",
    "plt.title(f'Wine - DBSCAN (Silhouette: {silhouette_dbscan_wine:.2f})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()\n",
    "\n",
    "# N칰mero de cl칰steres y puntos de ruido\n",
    "print('Iris - DBSCAN:')\n",
    "print('N췈 de cl칰steres:', len(set(labels_dbscan_iris)) - (1 if -1 in labels_dbscan_iris else 0))\n",
    "print('N췈 de puntos de ruido:', np.sum(labels_dbscan_iris == -1))\n",
    "print('---')\n",
    "print('Wine - DBSCAN:')\n",
    "print('N췈 de cl칰steres:', len(set(labels_dbscan_wine)) - (1 if -1 in labels_dbscan_wine else 0))\n",
    "print('N췈 de puntos de ruido:', np.sum(labels_dbscan_wine == -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592cd71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Comparaci칩n visual y cuantitativa de los resultados\n",
    "\n",
    "Vamos a comparar los resultados de los tres algoritmos en ambos datasets, tanto visualmente como usando m칠tricas como el Silhouette Score y el n칰mero de cl칰steres encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparativa de resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Algoritmo': ['K-Means', 'Jer치rquico', 'DBSCAN']*2,\n",
    "    'Dataset': ['Iris']*3 + ['Wine']*3,\n",
    "    'Silhouette': [silhouette_iris, silhouette_agg_iris, silhouette_dbscan_iris,\n",
    "                   silhouette_wine, silhouette_agg_wine, silhouette_dbscan_wine],\n",
    "    'N췈 Cl칰steres': [len(set(labels_iris)), len(set(labels_agg_iris)), len(set(labels_dbscan_iris)) - (1 if -1 in labels_dbscan_iris else 0),\n",
    "                     len(set(labels_wine)), len(set(labels_agg_wine)), len(set(labels_dbscan_wine)) - (1 if -1 in labels_dbscan_wine else 0)]\n",
    "})\n",
    "display(resultados)\n",
    "\n",
    "# Visualizaci칩n comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.barplot(data=resultados[resultados['Dataset']=='Iris'], x='Algoritmo', y='Silhouette', ax=axes[0])\n",
    "axes[0].set_title('Iris - Silhouette Score')\n",
    "sns.barplot(data=resultados[resultados['Dataset']=='Wine'], x='Algoritmo', y='Silhouette', ax=axes[1])\n",
    "axes[1].set_title('Wine - Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.barplot(data=resultados[resultados['Dataset']=='Iris'], x='Algoritmo', y='N췈 Cl칰steres', ax=axes[0])\n",
    "axes[0].set_title('Iris - N췈 Cl칰steres')\n",
    "sns.barplot(data=resultados[resultados['Dataset']=='Wine'], x='Algoritmo', y='N췈 Cl칰steres', ax=axes[1])\n",
    "axes[1].set_title('Wine - N췈 Cl칰steres')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d5639",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Ejemplo: Detecci칩n de anomal칤as con clustering\n",
    "\n",
    "El clustering tambi칠n puede ayudarnos a detectar anomal칤as. Los puntos etiquetados como ruido por DBSCAN suelen ser observaciones at칤picas. Veamos algunos ejemplos en los datos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecci칩n de anomal칤as en Iris\n",
    "anomalies_iris = df_iris_clean[labels_dbscan_iris == -1]\n",
    "print(f'Puntos de Iris detectados como anomal칤a por DBSCAN: {len(anomalies_iris)}')\n",
    "display(anomalies_iris)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], c='lightgray', s=40, label='Normal')\n",
    "plt.scatter(X_iris_pca[labels_dbscan_iris==-1,0], X_iris_pca[labels_dbscan_iris==-1,1], c='red', s=60, label='Anomal칤a')\n",
    "plt.title('Iris - Anomal칤as detectadas por DBSCAN')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Detecci칩n de anomal칤as en Wine\n",
    "anomalies_wine = df_wine_clean[labels_dbscan_wine == -1]\n",
    "print(f'Puntos de Wine detectados como anomal칤a por DBSCAN: {len(anomalies_wine)}')\n",
    "display(anomalies_wine)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_wine_pca[:,0], X_wine_pca[:,1], c='lightgray', s=40, label='Normal')\n",
    "plt.scatter(X_wine_pca[labels_dbscan_wine==-1,0], X_wine_pca[labels_dbscan_wine==-1,1], c='red', s=60, label='Anomal칤a')\n",
    "plt.title('Wine - Anomal칤as detectadas por DBSCAN')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b6571",
   "metadata": {},
   "source": [
    "## Comparaci칩n de diferentes algoritmos de agrupamiento en conjuntos de datos ficticios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413dd8e",
   "metadata": {},
   "source": [
    "Este ejemplo muestra las caracter칤sticas de diferentes algoritmos de agrupamiento en conjuntos de datos que son 춺interesantes췉, pero que siguen siendo bidimensionales. \n",
    "\n",
    "Con la excepci칩n del 칰ltimo conjunto de datos, los par치metros de cada uno de estos pares de conjuntos de datos y algoritmos se han ajustado para producir buenos resultados de agrupamiento. \n",
    "\n",
    "Algunos algoritmos son m치s sensibles a los valores de los par치metros que otros.\n",
    "\n",
    "El 칰ltimo conjunto de datos es un ejemplo de una situaci칩n 춺nula췉 para la agrupaci칩n: los datos son homog칠neos y no hay una buena agrupaci칩n. \n",
    "\n",
    "Para este ejemplo, el conjunto de datos nulo utiliza los mismos par치metros que el conjunto de datos de la fila superior, lo que representa una discrepancia entre los valores de los par치metros y la estructura de los datos.\n",
    "\n",
    "Si bien estos ejemplos dan una idea intuitiva de los algoritmos, es posible que esta intuici칩n no se aplique a datos de muy alta dimensi칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20330e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from itertools import cycle, islice\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============\n",
    "# Generar dataset. Elegimos un tama침o lo suficientemente grande como para ver la escalabilidad\n",
    "# de los algoritmos, pero no tan grande como para evitar tiempos de ejecuci칩n demasiado largos\n",
    "# ============\n",
    "n_samples = 500\n",
    "seed = 30\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n",
    ")\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "no_structure = rng.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "    \"allow_single_cluster\": True,\n",
    "    \"hdbscan_min_cluster_size\": 15,\n",
    "    \"hdbscan_min_samples\": 3,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        noisy_circles,\n",
    "        {\n",
    "            \"damping\": 0.77,\n",
    "            \"preference\": -240,\n",
    "            \"quantile\": 0.2,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.08,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        noisy_moons,\n",
    "        {\n",
    "            \"damping\": 0.75,\n",
    "            \"preference\": -220,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.1,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        varied,\n",
    "        {\n",
    "            \"eps\": 0.18,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.01,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        aniso,\n",
    "        {\n",
    "            \"eps\": 0.15,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 7,\n",
    "            \"xi\": 0.1,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # Actualizar par치metros con los espec칤ficos del dataset\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalizar el dataset para facilitar la selecci칩n de par치metros\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimar el bandwidth para mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # conectividad para structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # hacer sim칠trica la matriz de conectividad\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Crear los algoritmos de clustering\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n",
    "    )\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n",
    "    hdbscan = cluster.HDBSCAN(\n",
    "        min_samples=params[\"hdbscan_min_samples\"],\n",
    "        min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n",
    "        allow_single_cluster=params[\"allow_single_cluster\"],\n",
    "    )\n",
    "    optics = cluster.OPTICS(\n",
    "        min_samples=params[\"min_samples\"],\n",
    "        xi=params[\"xi\"],\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "    )\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params[\"damping\"],\n",
    "        preference=params[\"preference\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\",\n",
    "        metric=\"cityblock\",\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        connectivity=connectivity,\n",
    "    )\n",
    "    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params[\"n_clusters\"],\n",
    "        covariance_type=\"full\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        (\"MiniBatch\\nKMeans\", two_means),\n",
    "        (\"Affinity\\nPropagation\", affinity_propagation),\n",
    "        (\"MeanShift\", ms),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        (\"Ward\", ward),\n",
    "        (\"Agglomerative\\nClustering\", average_linkage),\n",
    "        (\"DBSCAN\", dbscan),\n",
    "        (\"HDBSCAN\", hdbscan),\n",
    "        (\"OPTICS\", optics),\n",
    "        (\"BIRCH\", birch),\n",
    "        (\"Gaussian\\nMixture\", gmm),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                \"connectivity matrix is [0-9]{1,2}\"\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\"\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()\n",
    "print(\"Este ejemplo visualiza varios algoritmos de clustering aplicados a diferentes datasets sint칠ticos.\")\n",
    "print(\"Nos puede ayudar a entender c칩mo funcionan y comparar su rendimiento visualmente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö Modelos Lineales con M√∫ltiples Par√°metros\n",
        "## Regresi√≥n Lineal y Clasificaci√≥n Log√≠stica\n",
        "\n",
        "**Curso:** IFCD093PO - Machine Learning\n",
        "\n",
        "**Objetivo:** Dominar modelos lineales con m√∫ltiples caracter√≠sticas y su interpretaci√≥n\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "from sklearn.metrics import (r2_score, mean_squared_error, accuracy_score,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             confusion_matrix, roc_curve, auc, classification_report)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('‚úÖ Todas las librer√≠as importadas correctamente')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üéØ TEOR√çA: REGRESI√ìN LINEAL vs CLASIFICACI√ìN LOG√çSTICA\n",
        "\n",
        "## Conceptos Fundamentales\n",
        "\n",
        "### üìà Regresi√≥n Lineal\n",
        "- **Objetivo:** Predecir valores **continuos** (n√∫meros reales)\n",
        "\n",
        "- **Ejemplos:** Precio de casa, temperatura, salario\n",
        "\n",
        "- **F√≥rmula:** $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$\n",
        "\n",
        "- **M√©trica:** R¬≤, RMSE\n",
        "\n",
        "### üéØ Clasificaci√≥n Log√≠stica\n",
        "- **Objetivo:** Predecir **categor√≠as binarias** (S√≠/No, 0/1)\n",
        "- **Ejemplos:** Spam/No spam, Enfermo/Sano, Aprobado/Reprobado\n",
        "- **Funci√≥n Sigmoide:** $P = \\frac{1}{1 + e^{-z}}$ donde $z = \\beta_0 + \\beta_1 x_1 + ...$\n",
        "- **Resultado:** Probabilidad entre 0 y 1\n",
        "- **M√©trica:** Exactitud, Precisi√≥n, Recall, F1-Score, AUC-ROC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('üéØ TEOR√çA: REGRESI√ìN LINEAL vs CLASIFICACI√ìN LOG√çSTICA\\\\n')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# REGRESI√ìN LINEAL\n",
        "np.random.seed(42)\n",
        "x_reg = np.linspace(0, 10, 100)\n",
        "y_reg = 2*x_reg + 1 + np.random.normal(0, 1, 100)\n",
        "axes[0].scatter(x_reg, y_reg, alpha=0.6, s=50)\n",
        "z_reg = np.polyfit(x_reg, y_reg, 1)\n",
        "p_reg = np.poly1d(z_reg)\n",
        "axes[0].plot(x_reg, p_reg(x_reg), 'r--', linewidth=2.5)\n",
        "axes[0].set_xlabel('Caracter√≠stica X', fontweight='bold')\n",
        "axes[0].set_ylabel('Target Y (continuo)', fontweight='bold')\n",
        "axes[0].set_title('REGRESI√ìN LINEAL\\\\nPredice valores continuos', fontweight='bold', fontsize=12)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# CLASIFICACI√ìN LOG√çSTICA\n",
        "x_clas = np.linspace(-3, 3, 100)\n",
        "y_prob = 1 / (1 + np.exp(-(2*x_clas)))\n",
        "y_clas = (np.random.random(len(x_clas)) < y_prob).astype(int)\n",
        "axes[1].scatter(x_clas, y_clas, alpha=0.6, s=50, c=y_clas, cmap='coolwarm')\n",
        "axes[1].plot(x_clas, y_prob, 'g-', linewidth=2.5, label='Sigmoide')\n",
        "axes[1].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
        "axes[1].set_xlabel('Caracter√≠stica X', fontweight='bold')\n",
        "axes[1].set_ylabel('Probabilidad', fontweight='bold')\n",
        "axes[1].set_title('REGRESI√ìN LOG√çSTICA\\\\nPredice probabilidades', fontweight='bold', fontsize=12)\n",
        "axes[1].set_ylim(-0.1, 1.1)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\\\nüí° DIFERENCIAS CLAVE:')\n",
        "print('   ‚Ä¢ Regresi√≥n: Predice valores continuos')\n",
        "print('   ‚Ä¢ Clasificaci√≥n: Predice probabilidades ‚Üí Categor√≠as')\n",
        "print('   ‚Ä¢ Ambos: Modelos LINEALES e INTERPRETABLES')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üè† EJERCICIO 1: CALIFORNIA HOUSING - REGRESI√ìN M√öLTIPLE\n",
        "\n",
        "**Objetivo:** Predecir precios de viviendas usando m√∫ltiples caracter√≠sticas\n",
        "\n",
        "**Dataset:** 20,640 casas de California con 8 caracter√≠sticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('üè† EJERCICIO 1: CALIFORNIA HOUSING - REGRESI√ìN M√öLTIPLE\\\\n')\n",
        "print('='*70)\n",
        "\n",
        "california = fetch_california_housing()\n",
        "df_california = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "df_california['MedHouseVal'] = california.target\n",
        "\n",
        "print('\\\\nüìä INFORMACI√ìN DEL DATASET:\\\\n')\n",
        "print(f'   Forma: {df_california.shape}')\n",
        "print(f'   Caracter√≠sticas: {list(df_california.columns)}')\n",
        "print(f'\\\\nüìà ESTAD√çSTICAS DESCRIPTIVAS:')\n",
        "print(df_california.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df_california.corr()\n",
        "sns.heatmap(corr_matrix[['MedHouseVal']].sort_values('MedHouseVal', ascending=False),\n",
        "            annot=True, cmap='coolwarm', center=0, fmt='.3f', linewidths=1)\n",
        "plt.title('Correlaci√≥n con Precio de Vivienda', fontweight='bold', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\\\nüîç CARACTER√çSTICAS M√ÅS CORRELACIONADAS:')\n",
        "corr_target = corr_matrix['MedHouseVal'].drop('MedHouseVal').sort_values(ascending=False)\n",
        "for feat, corr in corr_target.items():\n",
        "    print(f'   ‚Ä¢ {feat:20s}: {corr:+.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Regresi√≥n Simple vs M√∫ltiple\n",
        "\n",
        "Compararemos: **1 variable** vs **Todas las variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\\\n' + '='*70)\n",
        "print('COMPARANDO: REGRESI√ìN SIMPLE vs M√öLTIPLE')\n",
        "print('='*70)\n",
        "\n",
        "# REGRESI√ìN SIMPLE\n",
        "print('\\\\n1Ô∏è‚É£ REGRESI√ìN SIMPLE (MedInc solo)\\\\n')\n",
        "X_simple = df_california[['MedInc']]\n",
        "y = df_california['MedHouseVal']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_simple = LinearRegression()\n",
        "model_simple.fit(X_train, y_train)\n",
        "y_pred_simple = model_simple.predict(X_test)\n",
        "r2_simple = r2_score(y_test, y_pred_simple)\n",
        "rmse_simple = np.sqrt(mean_squared_error(y_test, y_pred_simple))\n",
        "\n",
        "print(f'   R¬≤ Score: {r2_simple:.4f}')\n",
        "print(f'   RMSE: {rmse_simple:.4f}')\n",
        "\n",
        "# REGRESI√ìN M√öLTIPLE\n",
        "print('\\\\n2Ô∏è‚É£ REGRESI√ìN M√öLTIPLE (8 variables)\\\\n')\n",
        "X_multiple = df_california.drop('MedHouseVal', axis=1)\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multiple, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_multiple = LinearRegression()\n",
        "model_multiple.fit(X_train_m, y_train_m)\n",
        "y_pred_multiple = model_multiple.predict(X_test_m)\n",
        "r2_multiple = r2_score(y_test_m, y_pred_multiple)\n",
        "rmse_multiple = np.sqrt(mean_squared_error(y_test_m, y_pred_multiple))\n",
        "\n",
        "print(f'   R¬≤ Score: {r2_multiple:.4f}')\n",
        "print(f'   RMSE: {rmse_multiple:.4f}')\n",
        "\n",
        "print(f'\\\\nüìä COMPARACI√ìN:')\n",
        "print(f'   R¬≤ Simple: {r2_simple:.4f} vs R¬≤ M√∫ltiple: {r2_multiple:.4f}')\n",
        "print(f'   Mejora: +{((r2_multiple - r2_simple) / r2_simple) * 100:.1f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "coef_df = pd.DataFrame({\n",
        "    'Caracter√≠stica': X_multiple.columns,\n",
        "    'Coeficiente': model_multiple.coef_\n",
        "}).sort_values('Coeficiente', ascending=True)\n",
        "\n",
        "colors = ['green' if x > 0 else 'red' for x in coef_df['Coeficiente']]\n",
        "axes[0].barh(coef_df['Caracter√≠stica'], coef_df['Coeficiente'], color=colors, alpha=0.7)\n",
        "axes[0].set_xlabel('Coeficiente', fontweight='bold')\n",
        "axes[0].set_title('Coeficientes\\\\n(Verde=Aumenta, Rojo=Disminuye)', fontweight='bold')\n",
        "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "models = ['Simple (1)', 'M√∫ltiple (8)']\n",
        "bars = axes[1].bar(models, [r2_simple, r2_multiple], color=['lightblue', 'lightgreen'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "axes[1].set_ylabel('R¬≤ Score', fontweight='bold')\n",
        "axes[1].set_title('Comparaci√≥n de Rendimiento', fontweight='bold')\n",
        "axes[1].set_ylim(0, 0.7)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, score in zip(bars, [r2_simple, r2_multiple]):\n",
        "    height = bar.get_height()\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\\\nüí° CONCLUSI√ìN: Las m√∫ltiples caracter√≠sticas mejoran significativamente el modelo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ü©∫ EJERCICIO 2: DIABETES - CLASIFICACI√ìN M√öLTIPLE\n",
        "\n",
        "**Objetivo:** Predecir si un paciente tiene diabetes avanzada\n",
        "\n",
        "**Dataset:** 442 pacientes con 10 caracter√≠sticas cl√≠nicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\\\nü©∫ EJERCICIO 2: DIABETES - CLASIFICACI√ìN M√öLTIPLE\\\\n')\n",
        "print('='*70)\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "df_diabetes = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "df_diabetes['progression'] = diabetes.target\n",
        "df_diabetes['diabetes_avanzada'] = (df_diabetes['progression'] > df_diabetes['progression'].median()).astype(int)\n",
        "\n",
        "print('\\\\nüìä INFORMACI√ìN DEL DATASET:')\n",
        "print(f'   Forma: {df_diabetes.shape}')\n",
        "print(f'   Proporci√≥n diabetes avanzada: {df_diabetes[\"diabetes_avanzada\"].mean()*100:.1f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_diab = df_diabetes.drop(['progression', 'diabetes_avanzada'], axis=1)\n",
        "y_diab = df_diabetes['diabetes_avanzada']\n",
        "\n",
        "X_diab_train, X_diab_test, y_diab_train, y_diab_test = train_test_split(\n",
        "    X_diab, y_diab, test_size=0.3, random_state=42, stratify=y_diab\n",
        ")\n",
        "\n",
        "scaler_diab = StandardScaler()\n",
        "X_diab_train_scaled = scaler_diab.fit_transform(X_diab_train)\n",
        "X_diab_test_scaled = scaler_diab.transform(X_diab_test)\n",
        "\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_diab_train_scaled, y_diab_train)\n",
        "\n",
        "y_pred_log = log_reg.predict(X_diab_test_scaled)\n",
        "accuracy = accuracy_score(y_diab_test, y_pred_log)\n",
        "f1 = f1_score(y_diab_test, y_pred_log)\n",
        "\n",
        "print(f'\\\\nüìä RESULTADOS:')\n",
        "print(f'   Exactitud: {accuracy:.4f} ({accuracy*100:.1f}%)')\n",
        "print(f'   F1-Score: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef_diabetes = pd.DataFrame({\n",
        "    'Caracter√≠stica': X_diab.columns,\n",
        "    'Coeficiente': log_reg.coef_[0],\n",
        "    'Odds Ratio': np.exp(log_reg.coef_[0]),\n",
        "    'Impacto': np.abs(log_reg.coef_[0])\n",
        "}).sort_values('Impacto', ascending=False)\n",
        "\n",
        "print(f'\\\\nüéØ COEFICIENTES (Top 5):')\n",
        "print(coef_diabetes[['Caracter√≠stica', 'Odds Ratio']].head().to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "colors = ['green' if x > 0 else 'red' for x in coef_diabetes['Coeficiente']]\n",
        "axes[0].barh(coef_diabetes['Caracter√≠stica'], coef_diabetes['Coeficiente'], color=colors, alpha=0.7)\n",
        "axes[0].set_xlabel('Coeficiente', fontweight='bold')\n",
        "axes[0].set_title('Coeficientes Log√≠sticos', fontweight='bold')\n",
        "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "cm = confusion_matrix(y_diab_test, y_pred_log)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1], cbar=False,\n",
        "            xticklabels=['No Avanzada', 'Avanzada'],\n",
        "            yticklabels=['No Avanzada', 'Avanzada'],\n",
        "            annot_kws={'fontsize': 12, 'fontweight': 'bold'})\n",
        "axes[1].set_title('Matriz de Confusi√≥n', fontweight='bold')\n",
        "axes[1].set_ylabel('Real', fontweight='bold')\n",
        "axes[1].set_xlabel('Predicci√≥n', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Regularizaci√≥n en Clasificaci√≥n\n",
        "\n",
        "Probaremos Ridge (L2) y Lasso (L1) para mejorar generalizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "log_reg_cv = LogisticRegression(random_state=42, max_iter=1000)\n",
        "grid_search = GridSearchCV(log_reg_cv, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_diab_train_scaled, y_diab_train)\n",
        "\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "y_pred_best = best_log_reg.predict(X_diab_test_scaled)\n",
        "accuracy_best = accuracy_score(y_diab_test, y_pred_best)\n",
        "\n",
        "print(f'‚úÖ Mejores par√°metros: {grid_search.best_params_}')\n",
        "print(f'üìä Exactitud original: {accuracy:.4f}')\n",
        "print(f'üìä Exactitud optimizada: {accuracy_best:.4f}')\n",
        "print(f'üéØ Mejora: +{(accuracy_best - accuracy)*100:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üö¢ EJERCICIO 3: TITANIC - CLASIFICACI√ìN CON FEATURE ENGINEERING\n",
        "\n",
        "**Objetivo:** Predecir supervivencia en el Titanic usando feature engineering\n",
        "\n",
        "**Desaf√≠o:** Crear caracter√≠sticas interpretables a partir de datos crudos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\\\nüö¢ EJERCICIO 3: TITANIC - CLASIFICACI√ìN CON FEATURE ENGINEERING\\\\n')\n",
        "print('='*70)\n",
        "\n",
        "try:\n",
        "    df_titanic = sns.load_dataset('titanic')\n",
        "except:\n",
        "    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "    df_titanic = pd.read_csv(url)\n",
        "\n",
        "print('\\\\nüìä INFORMACI√ìN INICIAL:')\n",
        "print(f'   Forma: {df_titanic.shape}')\n",
        "print(f'   Columnas: {list(df_titanic.columns)}')\n",
        "\n",
        "# Feature Engineering\n",
        "print('\\\\nüîß REALIZANDO FEATURE ENGINEERING...')\n",
        "df_titanic_clean = df_titanic.copy()\n",
        "\n",
        "df_titanic_clean['age'].fillna(df_titanic_clean['age'].median(), inplace=True)\n",
        "df_titanic_clean['embarked'].fillna(df_titanic_clean['embarked'].mode()[0], inplace=True)\n",
        "df_titanic_clean.drop(columns=['deck'], inplace=True, errors='ignore')\n",
        "\n",
        "df_titanic_clean['family_size'] = df_titanic_clean['sibsp'] + df_titanic_clean['parch'] + 1\n",
        "df_titanic_clean['is_alone'] = (df_titanic_clean['family_size'] == 1).astype(int)\n",
        "df_titanic_clean['title'] = df_titanic_clean['name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
        "\n",
        "title_mapping = {\n",
        "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
        "    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',\n",
        "    'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss', 'Lady': 'Rare',\n",
        "    'Jonkheer': 'Rare', 'Don': 'Rare', 'Dona': 'Rare', 'Mme': 'Mrs',\n",
        "    'Capt': 'Rare', 'Sir': 'Rare'\n",
        "}\n",
        "df_titanic_clean['title'] = df_titanic_clean['title'].map(title_mapping)\n",
        "\n",
        "# Codificar categ√≥ricas\n",
        "categorical_cols = ['sex', 'embarked', 'title', 'class', 'who', 'adult_male', 'embark_town', 'alive', 'alone']\n",
        "for col in categorical_cols:\n",
        "    if col in df_titanic_clean.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_titanic_clean[col] = le.fit_transform(df_titanic_clean[col].astype(str))\n",
        "\n",
        "features_titanic = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked',\n",
        "                    'family_size', 'is_alone', 'title', 'who', 'adult_male']\n",
        "X_titanic = df_titanic_clean[features_titanic]\n",
        "y_titanic = df_titanic_clean['survived']\n",
        "\n",
        "print(f'‚úÖ Feature Engineering completado')\n",
        "print(f'   Caracter√≠sticas: {len(features_titanic)}')\n",
        "print(f'   Tama√±o final: {X_titanic.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_titanic_train, X_titanic_test, y_titanic_train, y_titanic_test = train_test_split(\n",
        "    X_titanic, y_titanic, test_size=0.3, random_state=42, stratify=y_titanic\n",
        ")\n",
        "\n",
        "scaler_titanic = StandardScaler()\n",
        "X_titanic_train_scaled = scaler_titanic.fit_transform(X_titanic_train)\n",
        "X_titanic_test_scaled = scaler_titanic.transform(X_titanic_test)\n",
        "\n",
        "log_reg_titanic = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_titanic.fit(X_titanic_train_scaled, y_titanic_train)\n",
        "\n",
        "y_pred_titanic = log_reg_titanic.predict(X_titanic_test_scaled)\n",
        "y_pred_proba_titanic = log_reg_titanic.predict_proba(X_titanic_test_scaled)[:, 1]\n",
        "\n",
        "accuracy_titanic = accuracy_score(y_titanic_test, y_pred_titanic)\n",
        "f1_titanic = f1_score(y_titanic_test, y_pred_titanic)\n",
        "\n",
        "print(f'\\\\nüìä RESULTADOS - TITANIC:')\n",
        "print(f'   Exactitud: {accuracy_titanic:.4f} ({accuracy_titanic*100:.1f}%)')\n",
        "print(f'   F1-Score: {f1_titanic:.4f}')\n",
        "\n",
        "coef_titanic = pd.DataFrame({\n",
        "    'Caracter√≠stica': features_titanic,\n",
        "    'Coeficiente': log_reg_titanic.coef_[0],\n",
        "    'Odds Ratio': np.exp(log_reg_titanic.coef_[0]),\n",
        "    'Impacto': np.abs(log_reg_titanic.coef_[0])\n",
        "}).sort_values('Impacto', ascending=False)\n",
        "\n",
        "print(f'\\\\nüéØ TOP 5 CARACTER√çSTICAS M√ÅS IMPORTANTES:')\n",
        "print(coef_titanic[['Caracter√≠stica', 'Odds Ratio']].head().to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Coeficientes\n",
        "colors = ['green' if x > 0 else 'red' for x in coef_titanic['Coeficiente']]\n",
        "axes[0, 0].barh(coef_titanic['Caracter√≠stica'], coef_titanic['Coeficiente'], color=colors, alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Coeficiente', fontweight='bold')\n",
        "axes[0, 0].set_title('Coeficientes\\\\n(Verde=Aumenta supervivencia)', fontweight='bold')\n",
        "axes[0, 0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Odds Ratios\n",
        "colors = ['green' if x > 1 else 'red' for x in coef_titanic['Odds Ratio']]\n",
        "axes[0, 1].barh(coef_titanic['Caracter√≠stica'], coef_titanic['Odds Ratio'], color=colors, alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Odds Ratio', fontweight='bold')\n",
        "axes[0, 1].set_title('Odds Ratios\\\\n(>1=Mayor supervivencia)', fontweight='bold')\n",
        "axes[0, 1].axvline(x=1, color='black', linestyle='-', linewidth=1)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm_titanic = confusion_matrix(y_titanic_test, y_pred_titanic)\n",
        "sns.heatmap(cm_titanic, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0], cbar=False,\n",
        "            xticklabels=['Pred: No', 'Pred: S√≠'],\n",
        "            yticklabels=['Real: No', 'Real: S√≠'],\n",
        "            annot_kws={'fontsize': 12, 'fontweight': 'bold'})\n",
        "axes[1, 0].set_title('Matriz de Confusi√≥n', fontweight='bold')\n",
        "\n",
        "# Curva ROC\n",
        "fpr_titanic, tpr_titanic, _ = roc_curve(y_titanic_test, y_pred_proba_titanic)\n",
        "roc_auc_titanic = auc(fpr_titanic, tpr_titanic)\n",
        "axes[1, 1].plot(fpr_titanic, tpr_titanic, color='darkorange', lw=2.5, label=f'AUC={roc_auc_titanic:.3f}')\n",
        "axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "axes[1, 1].set_xlabel('Tasa de Falsos Positivos', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Tasa de Verdaderos Positivos', fontweight='bold')\n",
        "axes[1, 1].set_title('Curva ROC', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid_titanic = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "log_reg_titanic_cv = LogisticRegression(random_state=42, max_iter=1000)\n",
        "grid_search_titanic = GridSearchCV(log_reg_titanic_cv, param_grid_titanic, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_titanic.fit(X_titanic_train_scaled, y_titanic_train)\n",
        "\n",
        "best_log_reg_titanic = grid_search_titanic.best_estimator_\n",
        "y_pred_best_titanic = best_log_reg_titanic.predict(X_titanic_test_scaled)\n",
        "accuracy_best_titanic = accuracy_score(y_titanic_test, y_pred_best_titanic)\n",
        "\n",
        "print(f'‚úÖ Mejores par√°metros: {grid_search_titanic.best_params_}')\n",
        "print(f'üìä Exactitud original: {accuracy_titanic:.4f}')\n",
        "print(f'üìä Exactitud optimizada: {accuracy_best_titanic:.4f}')\n",
        "print(f'üéØ Mejora: +{(accuracy_best_titanic - accuracy_titanic)*100:.1f}%')\n",
        "\n",
        "cv_scores = cross_val_score(best_log_reg_titanic, X_titanic_train_scaled, y_titanic_train, cv=5, scoring='accuracy')\n",
        "print(f'\\\\nüîç Validaci√≥n cruzada (5-fold): {cv_scores.mean():.4f} +/- {cv_scores.std():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üìä RESUMEN COMPARATIVO FINAL\n",
        "\n",
        "Comparando resultados de los 3 ejercicios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\\\n' + '='*80)\n",
        "print('üìä RESUMEN COMPARATIVO FINAL')\n",
        "print('='*80)\n",
        "\n",
        "resumen = pd.DataFrame({\n",
        "    'Dataset': ['California', 'California', 'Diabetes', 'Diabetes', 'Titanic', 'Titanic'],\n",
        "    'Tipo': ['Regresi√≥n', 'Regresi√≥n', 'Clasificaci√≥n', 'Clasificaci√≥n', 'Clasificaci√≥n', 'Clasificaci√≥n'],\n",
        "    'Modelo': ['Simple', 'M√∫ltiple', 'Base', 'Optimizado', 'Base', 'Optimizado'],\n",
        "    'M√©trica': [r2_simple, r2_multiple, accuracy, accuracy_best, accuracy_titanic, accuracy_best_titanic]\n",
        "})\n",
        "\n",
        "print(resumen.to_string(index=False))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "axes[0].bar(['Simple', 'M√∫ltiple'], [r2_simple, r2_multiple], color=['lightblue', 'lightgreen'])\n",
        "axes[0].set_title('California Housing\\\\n(R¬≤)', fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1].bar(['Base', 'Optimizado'], [accuracy, accuracy_best], color=['lightblue', 'lightgreen'])\n",
        "axes[1].set_title('Diabetes\\\\n(Exactitud)', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[2].bar(['Base', 'Optimizado'], [accuracy_titanic, accuracy_best_titanic], color=['lightblue', 'lightgreen'])\n",
        "axes[2].set_title('Titanic\\\\n(Exactitud)', fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'''\\\\nüéØ CONCLUSIONES CLAVE:\\\\n\\\\n1. Regresi√≥n M√∫ltiple >> Regresi√≥n Simple\\\\n   Mejora: +{((r2_multiple - r2_simple) / r2_simple) * 100:.1f}%\\\\n\\\\n2. Optimizaci√≥n ayuda pero con retornos decrecientes\\\\n   Diabetes: +{(accuracy_best - accuracy)*100:.1f}%\\\\n   Titanic: +{(accuracy_best_titanic - accuracy_titanic)*100:.1f}%\\\\n\\\\n3. Feature engineering es CR√çTICO\\\\n   Titanic sin FE vs con FE: {accuracy_best_titanic*100:.0f}% de exactitud\\\\n\\\\n4. Modelos lineales son INTERPRETABLES\\\\n   Podemos entender cada decisi√≥n del modelo\\\\n\\\\n5. Siempre validar con datos independientes\\\\n   Validaci√≥n cruzada: Media ¬± Desv Est\\\\n''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üß™ EJERCICIOS ADICIONALES PARA PRACTICAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\\\n' + '='*80)\n",
        "print('üß™ EJERCICIOS ADICIONALES')\n",
        "print('='*80)\n",
        "\n",
        "ejercicios = '''\\\\nüéØ EJERCICIO 4: Interpretaci√≥n Profunda\n",
        "\\\\n1. Selecciona una casa/paciente/pasajero\n",
        "2. Explica c√≥mo cada caracter√≠stica contribuye a la predicci√≥n\n",
        "3. ¬øQu√© cambio ser√≠a necesario para cambiar la predicci√≥n?\n",
        "\n",
        "üîç EJERCICIO 5: An√°lisis de Errores\n",
        "\\\\n1. ¬øD√≥nde falla el modelo?\n",
        "2. ¬øHay patrones en los fallos?\n",
        "3. ¬øC√≥mo podr√≠as mejorar el feature engineering?\n",
        "\n",
        "üìà EJERCICIO 6: Experimentaci√≥n\n",
        "\\\\n1. Prueba diferentes regularizaciones (L1 vs L2)\n",
        "2. Crea interacciones entre variables\n",
        "3. Aplica transformaciones no lineales\n",
        "\n",
        "üöÄ EJERCICIO 7: Aplicaci√≥n Real\n",
        "\\\\n1. Encuentra un dataset similar\n",
        "2. Aplica el mismo pipeline completo\n",
        "3. Compara con nuestros resultados\n",
        "'''\n",
        "\n",
        "print(ejercicios)\n",
        "\n",
        "print(f'''\\\\nüéâ ¬°FELICITACIONES!\\\\n\\\\nHas aprendido:\\\\n‚úÖ Regresi√≥n Lineal M√∫ltiple\\\\n‚úÖ Regresi√≥n Log√≠stica para Clasificaci√≥n\\\\n‚úÖ Feature Engineering pr√°ctico\\\\n‚úÖ Regularizaci√≥n (Ridge/Lasso)\\\\n‚úÖ Optimizaci√≥n de Hiperpar√°metros\\\\n‚úÖ Evaluaci√≥n comprehensiva\\\\n‚úÖ Interpretaci√≥n de Modelos\\\\n\\\\n¬°Los modelos lineales son la BASE del ML!\\\\nDom√≠nalos antes de avanzar a t√©cnicas complejas. üöÄ\\\\n''')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
